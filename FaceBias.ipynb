{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiklasElsaesser/FaceBias/blob/main/FaceBias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rENlEcLVvNhb"
      },
      "source": [
        "# Project \"Bias in Face-Detection\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXslnT3veEX"
      },
      "source": [
        "## 1. Installing necessary libraries\n",
        "1.   OpenCV to preprocess the data (pictures)  \n",
        "2.   numpy to organize the data and link it with the labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeCr_-0pkArm",
        "outputId": "4450a2cc-9fa2-46d2-baa0-ea56002e4b94"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-contrib-python\n",
        "!pip install numpy as np\n",
        "!pip install tensorflow\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu44PSCyCHEb"
      },
      "source": [
        "## 2. Import the W&B library\n",
        "Initialize the W&B project and connecting it with the project on [W&B](https://wandb.ai/elsaesserniklas/4facesbias/reports/Face-Recognition-Bias-Report---Vmlldzo1NjUzNDg3/edit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "cW_35HYzCGCm",
        "outputId": "9154bc9e-2225-40aa-b419-cae0db5a4f8d"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project=\"4facesbias\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy2E9d8YzGip"
      },
      "source": [
        "## 3. Mounting the Google Drive\n",
        "The Pictures get uploaded for the labeling and preproccesing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5bCpgVqlx9H",
        "outputId": "fa895688-fcbd-47d9-9153-334444222136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvUITOar0VWB"
      },
      "source": [
        "## 4. Labeling & Preprocessing\n",
        "The pictures get loaded, preprocessed and labeled according to the folder names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z6ojkcvjBsr",
        "outputId": "f851df13-2855-4e85-c46b-939672e900b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "genders = [\"men\",\"women\"]\n",
        "emotions = [\"happy\",\"neutral\"]\n",
        "\n",
        "for gender in genders:\n",
        "    for emotion in emotions:\n",
        "        folder_path = f'/content/drive/MyDrive/Faces/Dataset/{gender}/{emotion}/'\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "            img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
        "            if img is None:\n",
        "              print(\"error\")\n",
        "              continue\n",
        "            img = cv2.resize(img, (48, 48))\n",
        "            data.append(img)\n",
        "            # Create labels by combining gender and emotion indices\n",
        "            label = genders.index(gender) * len(emotions) + emotions.index(emotion)\n",
        "            labels.append(label)\n",
        "\n",
        "data = np.array(data)\n",
        "data = data.reshape((data.shape[0], 48, 48, 1))\n",
        "labels = np.array(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmVeh2Z_8v8R"
      },
      "source": [
        "## 5. Preparing the Data\n",
        "Now Splitting the Data into a training and test set in an 80 - 20 config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xbDGakH6ot-e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3mHMxi189Uv"
      },
      "source": [
        "## 6. Training\n",
        "Building and training of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EKQWlxUoyaG",
        "outputId": "626efd04-bce8-4e51-e837-e1481afb5816"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential() # initializing a sequential model\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1))) #adding a 2D convolutional layer, the input shape is 48X48 and in grayscale, just as preprocessed\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # adding a 2D max-pooling layer with a size of 2X2 to reduce the previous laysers output by taking the max value\n",
        "model.add(Conv2D(128, (3, 3), activation='relu')) # adding another 2D convolutional layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # adding another 2D max-pooling layer\n",
        "model.add(Flatten()) # flattening the 2D vector into 1D to prepare it for the dense layer\n",
        "model.add(Dense(128, activation='relu')) # adding a dense (the fully connected) layer with 128 units\n",
        "model.add(Dense(len(genders) * len(emotions), activation='softmax'))  # output layer with the appropriate number of units for the amount of labels\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # configuring the model for training\n",
        "\n",
        "wandb_callback = wandb.keras.WandbCallback() # callback to monitor the data in WandB\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J72YNQXf33sR"
      },
      "outputs": [],
      "source": [
        "wandb_callback = wandb.keras.WandbCallback()\n",
        "#history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels), callbacks=[wandb_callback])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVP6xRy098ZD"
      },
      "source": [
        "## 8. Saving\n",
        "Saving the trained model to the GDrive, for an easier handling also as an .h5 file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_ZTklbno6ky",
        "outputId": "38074876-557a-4a09-f6fb-9ab6f1bf8fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the model to Google Drive\n",
        "model.save('bias_model.keras')\n",
        "model.save('/content/drive/MyDrive/Faces/Dataset/emotion_model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOZRGhI1H6akUPZTsHW+ib8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
