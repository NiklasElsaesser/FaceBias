{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMthdy9I6UM+mgbBq5jsmnw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiklasElsaesser/FaceRecognition/blob/main/FaceBias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project \"Bias in Face-Detection\"\n"
      ],
      "metadata": {
        "id": "rENlEcLVvNhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For labeling, the pictures have been structured in a folder structure which looks like this:\n",
        "\n",
        "dataset/\\\n",
        "├── men/\\\n",
        "│   ├── happy\\\n",
        "│   │   └── \\\n",
        "│   └── neutral/\\\n",
        "│       ├── neutral_jan1.jpg\\\n",
        "│       ├── neutral_niklas2.jpg \\\n",
        "│       └── ...\\\n",
        "└── women/\\\n",
        "    ├── happy/\\\n",
        "    │   ├── happy_woman1.jpg\\\n",
        "    │   ├── happy_woman2.jpg\\\n",
        "    │   └── ...\\\n",
        "    └── neutral/\\\n",
        "        └── \\\n"
      ],
      "metadata": {
        "id": "UmVsqs3SwRu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing necessary libraries\n",
        "1.   OpenCV to preprocess the data (pictures)  \n",
        "2.   numpy to organize the data and link it with the labels\n",
        "\n"
      ],
      "metadata": {
        "id": "PBXslnT3veEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python\n",
        "!pip install numpy as np\n",
        "!pip install tensorflow\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeCr_-0pkArm",
        "outputId": "368dd33c-15a9-434e-be65-1e568d4c5c55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python) (1.23.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for as\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=81cd4c3580c12f9fd53cba44e56a7195061a5a1bceeeed12de6cf173f49b1f9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.37 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the W&B library and initialize your W&B project at the beginning of your Colab notebook:"
      ],
      "metadata": {
        "id": "eu44PSCyCHEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"emotion-recognition-project\")"
      ],
      "metadata": {
        "id": "cW_35HYzCGCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting a Google Drive where the pictures are uploaded from."
      ],
      "metadata": {
        "id": "Jy2E9d8YzGip"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXmeRguLCEKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5bCpgVqlx9H",
        "outputId": "6230cbae-4a02-4fa8-c8ab-000443e3e804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the pictures get loaded and preprocessed."
      ],
      "metadata": {
        "id": "LvUITOar0VWB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8Z6ojkcvjBsr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "genders = [\"men\", \"women\"]\n",
        "emotions = [\"happy\", \"neutral\"]\n",
        "\n",
        "for gender in genders:\n",
        "    for emotion in emotions:\n",
        "        folder_path = f'/content/drive/MyDrive/Faces/Dataset/{gender}/{emotion}/'\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (48, 48))\n",
        "            data.append(img)\n",
        "            # Create labels by combining gender and emotion indices\n",
        "            label = genders.index(gender) * len(emotions) + emotions.index(emotion)\n",
        "            labels.append(label)\n",
        "\n",
        "data = np.array(data)\n",
        "data = data.reshape((data.shape[0], 48, 48, 1))\n",
        "labels = np.array(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Splitting the Data into a training and test set in an 80 - 20 config."
      ],
      "metadata": {
        "id": "dmVeh2Z_8v8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xbDGakH6ot-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and training of the model."
      ],
      "metadata": {
        "id": "D3mHMxi189Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(len(genders) * len(emotions), activation='softmax'))  # Output layer with appropriate number of units\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "wandb_callback = wandb.keras.WandbCallback()\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)"
      ],
      "metadata": {
        "id": "8EKQWlxUoyaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking for loss and accuracy."
      ],
      "metadata": {
        "id": "lpNjqqih93X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss:.2f}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "WceiBKeYo36A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the trained model to the GDrive"
      ],
      "metadata": {
        "id": "IVP6xRy098ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the model to Google Drive\n",
        "model.save('/content/drive/MyDrive/Faces/Dataset/emotion_model.h5')"
      ],
      "metadata": {
        "id": "n_ZTklbno6ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "allow for the input of a new picture on which the algorithm wasnt trained to test for the bias of the algorithm."
      ],
      "metadata": {
        "id": "BYeus_75_YYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/Faces/Dataset/emotion_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Function to preprocess uploaded images\n",
        "def preprocess_image(file_path):\n",
        "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (48, 48))\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img.reshape((1, 48, 48, 1))\n",
        "    return img\n",
        "\n",
        "# Function to predict emotion based on the uploaded image\n",
        "def predict_emotion(file_path):\n",
        "    preprocessed_img = preprocess_image(file_path)\n",
        "    prediction = model.predict(preprocessed_img)\n",
        "    gender_index, emotion_index = np.unravel_index(np.argmax(prediction, axis=None), prediction.shape)\n",
        "    gender = genders[gender_index]\n",
        "    emotion = emotions[emotion_index % len(emotions)]\n",
        "    return gender, emotion\n",
        "\n",
        "# Upload your own picture\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process and predict emotions for the uploaded image(s)\n",
        "for file_name in uploaded.keys():\n",
        "    file_path = '/content/' + file_name #add correct file path\n",
        "    gender, emotion = predict_emotion(file_path)\n",
        "    print(f\"Predicted Gender: {gender.capitalize()}, Predicted Emotion: {emotion.capitalize()}\")\n"
      ],
      "metadata": {
        "id": "Aw5j1ni4phcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}